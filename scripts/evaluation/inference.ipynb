{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T09:00:52.655051Z",
     "start_time": "2025-07-01T09:00:52.652585Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/iusers01/fatpou01/compsci01/b08593hm/.conda/envs/tooncrafter/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T09:00:53.576673Z",
     "start_time": "2025-07-01T09:00:53.573534Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/iusers01/fatpou01/compsci01/b08593hm/.conda/envs/tooncrafter/lib/python3.8/site-packages/ipykernel_launcher.py'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "os.path.abspath(sys.argv[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T09:00:54.164296Z",
     "start_time": "2025-07-01T09:00:54.161775Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "/opt/apps/libs/nvidia-cuda/tensorrt/7.2.2/11.1/python/lib/python3.9/site-packages\n",
      "/mnt/iusers01/fatpou01/compsci01/b08593hm/.conda/envs/tooncrafter/lib/python38.zip\n",
      "/mnt/iusers01/fatpou01/compsci01/b08593hm/.conda/envs/tooncrafter/lib/python3.8\n",
      "/mnt/iusers01/fatpou01/compsci01/b08593hm/.conda/envs/tooncrafter/lib/python3.8/lib-dynload\n",
      "/mnt/iusers01/fatpou01/compsci01/b08593hm/.local/lib/python3.8/site-packages\n",
      "/mnt/iusers01/fatpou01/compsci01/b08593hm/.conda/envs/tooncrafter/lib/python3.8/site-packages\n",
      "apps/binapps/pytorch/2.3.0-311-cpu\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "for p in sys.path:\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T09:01:08.001379Z",
     "start_time": "2025-07-01T09:00:54.997888Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import argparse, os, sys, glob\n",
    "import datetime, time\n",
    "from omegaconf import OmegaConf\n",
    "from tqdm import tqdm\n",
    "from einops import rearrange, repeat\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from pytorch_lightning import seed_everything\n",
    "from PIL import Image\n",
    "sys.path.insert(1, os.path.join(sys.path[0], '..', '..'))\n",
    "from lvdm.models.samplers.ddim import DDIMSampler\n",
    "from lvdm.models.samplers.ddim_multiplecond import DDIMSampler as DDIMSampler_multicond\n",
    "from utils.utils import instantiate_from_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T09:01:08.049593Z",
     "start_time": "2025-07-01T09:01:08.004441Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_filelist(data_dir, postfixes):\n",
    "    patterns = [os.path.join(data_dir, f\"*.{postfix}\") for postfix in postfixes]\n",
    "    file_list = []\n",
    "    for pattern in patterns:\n",
    "        file_list.extend(glob.glob(pattern))\n",
    "    file_list.sort()\n",
    "    return file_list\n",
    "\n",
    "def load_model_checkpoint(model, ckpt):\n",
    "    state_dict = torch.load(ckpt, map_location=\"cpu\")\n",
    "    if \"state_dict\" in list(state_dict.keys()):\n",
    "        state_dict = state_dict[\"state_dict\"]\n",
    "        try:\n",
    "            model.load_state_dict(state_dict, strict=True)\n",
    "        except:\n",
    "            ## rename the keys for 256x256 model\n",
    "            new_pl_sd = OrderedDict()\n",
    "            for k,v in state_dict.items():\n",
    "                new_pl_sd[k] = v\n",
    "\n",
    "            for k in list(new_pl_sd.keys()):\n",
    "                if \"framestride_embed\" in k:\n",
    "                    new_key = k.replace(\"framestride_embed\", \"fps_embedding\")\n",
    "                    new_pl_sd[new_key] = new_pl_sd[k]\n",
    "                    del new_pl_sd[k]\n",
    "            model.load_state_dict(new_pl_sd, strict=True)\n",
    "    else:\n",
    "        # deepspeed\n",
    "        new_pl_sd = OrderedDict()\n",
    "        for key in state_dict['module'].keys():\n",
    "            new_pl_sd[key[16:]]=state_dict['module'][key]\n",
    "        model.load_state_dict(new_pl_sd)\n",
    "    print('>>> model checkpoint loaded.')\n",
    "    return model\n",
    "\n",
    "def load_prompts(prompt_file):\n",
    "    f = open(prompt_file, 'r')\n",
    "    prompt_list = []\n",
    "    for idx, line in enumerate(f.readlines()):\n",
    "        l = line.strip()\n",
    "        if len(l) != 0:\n",
    "            prompt_list.append(l)\n",
    "        f.close()\n",
    "    return prompt_list\n",
    "\n",
    "def load_data_prompts(data_dir, video_size=(256,256), video_frames=16, interp=False):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(min(video_size)),\n",
    "        transforms.CenterCrop(video_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))])\n",
    "    ## load prompts\n",
    "    prompt_file = get_filelist(data_dir, ['txt'])\n",
    "    assert len(prompt_file) > 0, \"Error: found NO prompt file!\"\n",
    "    ###### default prompt\n",
    "    default_idx = 0\n",
    "    default_idx = min(default_idx, len(prompt_file)-1)\n",
    "    if len(prompt_file) > 1:\n",
    "        print(f\"Warning: multiple prompt files exist. The one {os.path.split(prompt_file[default_idx])[1]} is used.\")\n",
    "    ## only use the first one (sorted by name) if multiple exist\n",
    "    \n",
    "    ## load video\n",
    "    file_list = get_filelist(data_dir, ['jpg', 'png', 'jpeg', 'JPEG', 'PNG'])\n",
    "    # assert len(file_list) == n_samples, \"Error: data and prompts are NOT paired!\"\n",
    "    data_list = []\n",
    "    filename_list = []\n",
    "    prompt_list = load_prompts(prompt_file[default_idx])\n",
    "    n_samples = len(prompt_list)\n",
    "    for idx in range(n_samples):\n",
    "        if interp:\n",
    "            image1 = Image.open(file_list[2*idx]).convert('RGB')\n",
    "            image_tensor1 = transform(image1).unsqueeze(1) # [c,1,h,w]\n",
    "            image2 = Image.open(file_list[2*idx+1]).convert('RGB')\n",
    "            image_tensor2 = transform(image2).unsqueeze(1) # [c,1,h,w]\n",
    "            frame_tensor1 = repeat(image_tensor1, 'c t h w -> c (repeat t) h w', repeat=video_frames//2)\n",
    "            frame_tensor2 = repeat(image_tensor2, 'c t h w -> c (repeat t) h w', repeat=video_frames//2)\n",
    "            frame_tensor = torch.cat([frame_tensor1, frame_tensor2], dim=1)\n",
    "            _, filename = os.path.split(file_list[idx*2])\n",
    "        else:\n",
    "            image = Image.open(file_list[idx]).convert('RGB')\n",
    "            image_tensor = transform(image).unsqueeze(1) # [c,1,h,w]\n",
    "            frame_tensor = repeat(image_tensor, 'c t h w -> c (repeat t) h w', repeat=video_frames)\n",
    "            _, filename = os.path.split(file_list[idx])\n",
    "\n",
    "        data_list.append(frame_tensor)\n",
    "        filename_list.append(filename)\n",
    "        \n",
    "    return filename_list, data_list, prompt_list\n",
    "\n",
    "\n",
    "def save_results(prompt, samples, filename, fakedir, fps=8, loop=False):\n",
    "    filename = filename.split('.')[0]+'.mp4'\n",
    "    prompt = prompt[0] if isinstance(prompt, list) else prompt\n",
    "\n",
    "    ## save video\n",
    "    videos = [samples]\n",
    "    savedirs = [fakedir]\n",
    "    for idx, video in enumerate(videos):\n",
    "        if video is None:\n",
    "            continue\n",
    "        # b,c,t,h,w\n",
    "        video = video.detach().cpu()\n",
    "        video = torch.clamp(video.float(), -1., 1.)\n",
    "        n = video.shape[0]\n",
    "        video = video.permute(2, 0, 1, 3, 4) # t,n,c,h,w\n",
    "        if loop:\n",
    "            video = video[:-1,...]\n",
    "        \n",
    "        frame_grids = [torchvision.utils.make_grid(framesheet, nrow=int(n), padding=0) for framesheet in video] #[3, 1*h, n*w]\n",
    "        grid = torch.stack(frame_grids, dim=0) # stack in temporal dim [t, 3, h, n*w]\n",
    "        grid = (grid + 1.0) / 2.0\n",
    "        grid = (grid * 255).to(torch.uint8).permute(0, 2, 3, 1)\n",
    "        path = os.path.join(savedirs[idx], filename)\n",
    "        torchvision.io.write_video(path, grid, fps=fps, video_codec='h264', options={'crf': '10'}) ## crf indicates the quality\n",
    "\n",
    "\n",
    "def save_results_seperate(prompt, samples, filename, fakedir, fps=10, loop=False):\n",
    "    prompt = prompt[0] if isinstance(prompt, list) else prompt\n",
    "\n",
    "    ## save video\n",
    "    videos = [samples]\n",
    "    savedirs = [fakedir]\n",
    "    for idx, video in enumerate(videos):\n",
    "        if video is None:\n",
    "            continue\n",
    "        # b,c,t,h,w\n",
    "        video = video.detach().cpu()\n",
    "        if loop: # remove the last frame\n",
    "            video = video[:,:,:-1,...]\n",
    "        video = torch.clamp(video.float(), -1., 1.)\n",
    "        n = video.shape[0]\n",
    "        for i in range(n):\n",
    "            grid = video[i,...]\n",
    "            grid = (grid + 1.0) / 2.0\n",
    "            grid = (grid * 255).to(torch.uint8).permute(1, 2, 3, 0) #thwc\n",
    "            path = os.path.join(savedirs[idx].replace('samples', 'samples_separate'), f'{filename.split(\".\")[0]}_sample{i}.mp4')\n",
    "            torchvision.io.write_video(path, grid, fps=fps, video_codec='h264', options={'crf': '10'})\n",
    "\n",
    "def get_latent_z(model, videos):\n",
    "    b, c, t, h, w = videos.shape\n",
    "    x = rearrange(videos, 'b c t h w -> (b t) c h w')\n",
    "    z = model.encode_first_stage(x)\n",
    "    z = rearrange(z, '(b t) c h w -> b c t h w', b=b, t=t)\n",
    "    return z\n",
    "\n",
    "def get_latent_z_with_hidden_states(model, videos):\n",
    "    b, c, t, h, w = videos.shape\n",
    "    x = rearrange(videos, 'b c t h w -> (b t) c h w')\n",
    "    encoder_posterior, hidden_states = model.first_stage_model.encode(x, return_hidden_states=True)\n",
    "\n",
    "    hidden_states_first_last = []\n",
    "    ### use only the first and last hidden states\n",
    "    for hid in hidden_states:\n",
    "        hid = rearrange(hid, '(b t) c h w -> b c t h w', t=t)\n",
    "        hid_new = torch.cat([hid[:, :, 0:1], hid[:, :, -1:]], dim=2)\n",
    "        hidden_states_first_last.append(hid_new)\n",
    "\n",
    "    z = model.get_first_stage_encoding(encoder_posterior).detach()\n",
    "    z = rearrange(z, '(b t) c h w -> b c t h w', b=b, t=t)\n",
    "    return z, hidden_states_first_last\n",
    "\n",
    "def image_guided_synthesis(model, prompts, videos, noise_shape, n_samples=1, ddim_steps=50, ddim_eta=1., \\\n",
    "                        unconditional_guidance_scale=1.0, cfg_img=None, fs=None, text_input=False, multiple_cond_cfg=False, loop=False, interp=False, timestep_spacing='uniform', guidance_rescale=0.0, **kwargs):\n",
    "    ddim_sampler = DDIMSampler(model) if not multiple_cond_cfg else DDIMSampler_multicond(model)\n",
    "    batch_size = noise_shape[0]\n",
    "    fs = torch.tensor([fs] * batch_size, dtype=torch.long, device=model.device)\n",
    "\n",
    "    if not text_input:\n",
    "        prompts = [\"\"]*batch_size\n",
    "\n",
    "    img = videos[:,:,0] #bchw\n",
    "    img_emb = model.embedder(img) ## blc\n",
    "    img_emb = model.image_proj_model(img_emb)\n",
    "\n",
    "    cond_emb = model.get_learned_conditioning(prompts)\n",
    "    cond = {\"c_crossattn\": [torch.cat([cond_emb,img_emb], dim=1)]}\n",
    "    if model.model.conditioning_key == 'hybrid':\n",
    "        z, hs = get_latent_z_with_hidden_states(model, videos) # b c t h w\n",
    "        if loop or interp:\n",
    "            img_cat_cond = torch.zeros_like(z)\n",
    "            img_cat_cond[:,:,0,:,:] = z[:,:,0,:,:]\n",
    "            img_cat_cond[:,:,-1,:,:] = z[:,:,-1,:,:]\n",
    "        else:\n",
    "            img_cat_cond = z[:,:,:1,:,:]\n",
    "            img_cat_cond = repeat(img_cat_cond, 'b c t h w -> b c (repeat t) h w', repeat=z.shape[2])\n",
    "        cond[\"c_concat\"] = [img_cat_cond] # b c 1 h w\n",
    "    \n",
    "    if unconditional_guidance_scale != 1.0:\n",
    "        if model.uncond_type == \"empty_seq\":\n",
    "            prompts = batch_size * [\"\"]\n",
    "            uc_emb = model.get_learned_conditioning(prompts)\n",
    "        elif model.uncond_type == \"zero_embed\":\n",
    "            uc_emb = torch.zeros_like(cond_emb)\n",
    "        uc_img_emb = model.embedder(torch.zeros_like(img)) ## b l c\n",
    "        uc_img_emb = model.image_proj_model(uc_img_emb)\n",
    "        uc = {\"c_crossattn\": [torch.cat([uc_emb,uc_img_emb],dim=1)]}\n",
    "        if model.model.conditioning_key == 'hybrid':\n",
    "            uc[\"c_concat\"] = [img_cat_cond]\n",
    "    else:\n",
    "        uc = None\n",
    "\n",
    "    additional_decode_kwargs = {'ref_context': hs}\n",
    "\n",
    "    ## we need one more unconditioning image=yes, text=\"\"\n",
    "    if multiple_cond_cfg and cfg_img != 1.0:\n",
    "        uc_2 = {\"c_crossattn\": [torch.cat([uc_emb,img_emb],dim=1)]}\n",
    "        if model.model.conditioning_key == 'hybrid':\n",
    "            uc_2[\"c_concat\"] = [img_cat_cond]\n",
    "        kwargs.update({\"unconditional_conditioning_img_nonetext\": uc_2})\n",
    "    else:\n",
    "        kwargs.update({\"unconditional_conditioning_img_nonetext\": None})\n",
    "\n",
    "    z0 = None\n",
    "    cond_mask = None\n",
    "\n",
    "    batch_variants = []\n",
    "    for _ in range(n_samples):\n",
    "\n",
    "        if z0 is not None:\n",
    "            cond_z0 = z0.clone()\n",
    "            kwargs.update({\"clean_cond\": True})\n",
    "        else:\n",
    "            cond_z0 = None\n",
    "        if ddim_sampler is not None:\n",
    "\n",
    "            samples, _ = ddim_sampler.sample(S=ddim_steps,\n",
    "                                            conditioning=cond,\n",
    "                                            batch_size=batch_size,\n",
    "                                            shape=noise_shape[1:],\n",
    "                                            verbose=False,\n",
    "                                            unconditional_guidance_scale=unconditional_guidance_scale,\n",
    "                                            unconditional_conditioning=uc,\n",
    "                                            eta=ddim_eta,\n",
    "                                            cfg_img=cfg_img, \n",
    "                                            mask=cond_mask,\n",
    "                                            x0=cond_z0,\n",
    "                                            fs=fs,\n",
    "                                            timestep_spacing=timestep_spacing,\n",
    "                                            guidance_rescale=guidance_rescale,\n",
    "                                            **kwargs\n",
    "                                            )\n",
    "\n",
    "        ## reconstruct from latent to pixel space\n",
    "        batch_images = model.decode_first_stage(samples, **additional_decode_kwargs)\n",
    "\n",
    "        index = list(range(samples.shape[2]))\n",
    "        del index[1]\n",
    "        del index[-2]\n",
    "        samples = samples[:,:,index,:,:]\n",
    "        ## reconstruct from latent to pixel space\n",
    "        batch_images_middle = model.decode_first_stage(samples, **additional_decode_kwargs)\n",
    "        batch_images[:,:,batch_images.shape[2]//2-1:batch_images.shape[2]//2+1] = batch_images_middle[:,:,batch_images.shape[2]//2-2:batch_images.shape[2]//2]\n",
    "\n",
    "\n",
    "\n",
    "        batch_variants.append(batch_images)\n",
    "    ## variants, batch, c, t, h, w\n",
    "    batch_variants = torch.stack(batch_variants)\n",
    "    return batch_variants.permute(1, 0, 2, 3, 4, 5)\n",
    "\n",
    "\n",
    "def run_inference(args, gpu_num, gpu_no):\n",
    "    ## model config\n",
    "    config = OmegaConf.load(args.config)\n",
    "    model_config = config.pop(\"model\", OmegaConf.create())\n",
    "    \n",
    "    ## set use_checkpoint as False as when using deepspeed, it encounters an error \"deepspeed backend not set\"\n",
    "    model_config['params']['unet_config']['params']['use_checkpoint'] = False\n",
    "    model = instantiate_from_config(model_config)\n",
    "    model = model.cuda(gpu_no)\n",
    "    model.perframe_ae = args.perframe_ae\n",
    "    assert os.path.exists(args.ckpt_path), \"Error: checkpoint Not Found!\"\n",
    "    model = load_model_checkpoint(model, args.ckpt_path)\n",
    "    model.eval()\n",
    "\n",
    "    ## run over data\n",
    "    assert (args.height % 16 == 0) and (args.width % 16 == 0), \"Error: image size [h,w] should be multiples of 16!\"\n",
    "    assert args.bs == 1, \"Current implementation only support [batch size = 1]!\"\n",
    "    ## latent noise shape\n",
    "    h, w = args.height // 8, args.width // 8\n",
    "    channels = model.model.diffusion_model.out_channels\n",
    "    n_frames = args.video_length\n",
    "    print(f'Inference with {n_frames} frames')\n",
    "    noise_shape = [args.bs, channels, n_frames, h, w]\n",
    "\n",
    "    fakedir = os.path.join(args.savedir, \"samples\")\n",
    "    fakedir_separate = os.path.join(args.savedir, \"samples_separate\")\n",
    "\n",
    "    # os.makedirs(fakedir, exist_ok=True)\n",
    "    os.makedirs(fakedir_separate, exist_ok=True)\n",
    "\n",
    "    ## prompt file setting\n",
    "    assert os.path.exists(args.prompt_dir), \"Error: prompt file Not Found!\"\n",
    "    filename_list, data_list, prompt_list = load_data_prompts(args.prompt_dir, video_size=(args.height, args.width), video_frames=n_frames, interp=args.interp)\n",
    "    num_samples = len(prompt_list)\n",
    "    samples_split = num_samples // gpu_num\n",
    "    print('Prompts testing [rank:%d] %d/%d samples loaded.'%(gpu_no, samples_split, num_samples))\n",
    "    #indices = random.choices(list(range(0, num_samples)), k=samples_per_device)\n",
    "    indices = list(range(samples_split*gpu_no, samples_split*(gpu_no+1)))\n",
    "    prompt_list_rank = [prompt_list[i] for i in indices]\n",
    "    data_list_rank = [data_list[i] for i in indices]\n",
    "    filename_list_rank = [filename_list[i] for i in indices]\n",
    "\n",
    "    start = time.time()\n",
    "    with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "        for idx, indice in tqdm(enumerate(range(0, len(prompt_list_rank), args.bs)), desc='Sample Batch'):\n",
    "            prompts = prompt_list_rank[indice:indice+args.bs]\n",
    "            videos = data_list_rank[indice:indice+args.bs]\n",
    "            filenames = filename_list_rank[indice:indice+args.bs]\n",
    "            print(prompts)\n",
    "            print(videos)\n",
    "            print(filenames)\n",
    "            \n",
    "            device = torch.device(f'cuda:{gpu_no}')\n",
    "            if isinstance(videos, list):\n",
    "                videos = torch.stack(videos, dim=0).to(device)\n",
    "            else:\n",
    "                videos = videos.unsqueeze(0).to(device)\n",
    "\n",
    "            batch_samples = image_guided_synthesis(model, prompts, videos, noise_shape, args.n_samples, args.ddim_steps, args.ddim_eta, \\\n",
    "                                args.unconditional_guidance_scale, args.cfg_img, args.frame_stride, args.text_input, args.multiple_cond_cfg, args.loop, args.interp, args.timestep_spacing, args.guidance_rescale)\n",
    "\n",
    "            ## save each example individually\n",
    "            for nn, samples in enumerate(batch_samples):\n",
    "                ## samples : [n_samples,c,t,h,w]\n",
    "                prompt = prompts[nn]\n",
    "                filename = filenames[nn]\n",
    "                save_results(prompt, samples, filename, fakedir, fps=8, loop=args.loop)\n",
    "#                 save_results_seperate(prompt, samples, filename, fakedir, fps=8, loop=args.loop)\n",
    "\n",
    "    print(f\"Saved in {args.savedir}. Time used: {(time.time() - start):.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T09:01:08.078900Z",
     "start_time": "2025-07-01T09:01:08.051300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@DynamiCrafter cond-Inference: 2025-07-01-10-01-08\n"
     ]
    }
   ],
   "source": [
    "now = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "print(\"@DynamiCrafter cond-Inference: %s\"%now)\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--savedir\", type=str, default=None, help=\"results saving path\")\n",
    "parser.add_argument(\"--ckpt_path\", type=str, default=None, help=\"checkpoint path\")\n",
    "parser.add_argument(\"--config\", type=str, help=\"config (yaml) path\")\n",
    "parser.add_argument(\"--prompt_dir\", type=str, default=None, help=\"a data dir containing videos and prompts\")\n",
    "parser.add_argument(\"--n_samples\", type=int, default=1, help=\"num of samples per prompt\",)\n",
    "parser.add_argument(\"--ddim_steps\", type=int, default=50, help=\"steps of ddim if positive, otherwise use DDPM\",)\n",
    "parser.add_argument(\"--ddim_eta\", type=float, default=1.0, help=\"eta for ddim sampling (0.0 yields deterministic sampling)\",)\n",
    "parser.add_argument(\"--bs\", type=int, default=1, help=\"batch size for inference, should be one\")\n",
    "parser.add_argument(\"--height\", type=int, default=512, help=\"image height, in pixel space\")\n",
    "parser.add_argument(\"--width\", type=int, default=512, help=\"image width, in pixel space\")\n",
    "parser.add_argument(\"--frame_stride\", type=int, default=3, help=\"frame stride control for 256 model (larger->larger motion), FPS control for 512 or 1024 model (smaller->larger motion)\")\n",
    "parser.add_argument(\"--unconditional_guidance_scale\", type=float, default=1.0, help=\"prompt classifier-free guidance\")\n",
    "parser.add_argument(\"--seed\", type=int, default=123, help=\"seed for seed_everything\")\n",
    "parser.add_argument(\"--video_length\", type=int, default=16, help=\"inference video length\")\n",
    "parser.add_argument(\"--negative_prompt\", action='store_true', default=False, help=\"negative prompt\")\n",
    "parser.add_argument(\"--text_input\", action='store_true', default=False, help=\"input text to I2V model or not\")\n",
    "parser.add_argument(\"--multiple_cond_cfg\", action='store_true', default=False, help=\"use multi-condition cfg or not\")\n",
    "parser.add_argument(\"--cfg_img\", type=float, default=None, help=\"guidance scale for image conditioning\")\n",
    "parser.add_argument(\"--timestep_spacing\", type=str, default=\"uniform\", help=\"The way the timesteps should be scaled. Refer to Table 2 of the [Common Diffusion Noise Schedules and Sample Steps are Flawed](https://huggingface.co/papers/2305.08891) for more information.\")\n",
    "parser.add_argument(\"--guidance_rescale\", type=float, default=0.0, help=\"guidance rescale in [Common Diffusion Noise Schedules and Sample Steps are Flawed](https://huggingface.co/papers/2305.08891)\")\n",
    "parser.add_argument(\"--perframe_ae\", action='store_true', default=False, help=\"if we use per-frame AE decoding, set it to True to save GPU memory, especially for the model of 576x1024\")\n",
    "\n",
    "## currently not support looping video and generative frame interpolation\n",
    "parser.add_argument(\"--loop\", action='store_true', default=False, help=\"generate looping videos or not\")\n",
    "parser.add_argument(\"--interp\", action='store_true', default=False, help=\"generate generative frame interpolation or not\")\n",
    "\n",
    "args, unknown = parser.parse_known_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T09:01:08.098025Z",
     "start_time": "2025-07-01T09:01:08.080869Z"
    }
   },
   "outputs": [],
   "source": [
    "args.seed = 123\n",
    "args.ckpt_path = \"../../checkpoints/tooncrafter_512_interp_v1/model.ckpt\"\n",
    "args.config = \"../../configs/inference_512_v1.0.yaml\"\n",
    "args.savedir = f\"results/tooncrafter_512_interp_seed{args.seed}\" \n",
    "args.n_samples = 1 \n",
    "args.bs = 1\n",
    "args.height = 80\n",
    "args.width = 128\n",
    "args.unconditional_guidance_scale = 7.5\n",
    "args.ddim_steps = 50 \n",
    "args.ddim_eta = 1.0 \n",
    "# args.prompt_dir = \"../../prompts/my_tests\" \n",
    "args.prompt_dir = \"../../prompts/512_interp/\"\n",
    "args.text_input = True\n",
    "args.video_length = 16\n",
    "args.frame_stride = 10\n",
    "args.timestep_spacing = 'uniform_trailing' \n",
    "args.guidance_rescale = 0.7 \n",
    "args.perframe_ae = True\n",
    "args.interp = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T09:05:02.635189Z",
     "start_time": "2025-07-01T09:02:27.786174Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank: 0] Global seed set to 123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AE working on z of shape (1, 4, 32, 32) = 4096 dimensions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/iusers01/fatpou01/compsci01/b08593hm/.conda/envs/tooncrafter/lib/python3.8/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
      "/tmp/slurm.2937341/ipykernel_2769655/262529346.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(ckpt, map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> model checkpoint loaded.\n",
      "Inference with 16 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/slurm.2937341/ipykernel_2769655/262529346.py:305: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.no_grad(), torch.cuda.amp.autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompts testing [rank:0] 1/1 samples loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Sample Batch: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['an anime scene']\n",
      "[tensor([[[[ 0.0902, -0.0588, -0.1765,  ..., -0.5137, -0.5686, -0.4667],\n",
      "          [ 0.0902, -0.0353, -0.1373,  ..., -0.5059, -0.5137, -0.4431],\n",
      "          [ 0.0745, -0.0118, -0.0902,  ..., -0.3647, -0.4039, -0.3569],\n",
      "          ...,\n",
      "          [ 0.6235,  0.6078,  0.6000,  ...,  0.2000,  0.1608,  0.1451],\n",
      "          [ 0.6078,  0.5922,  0.5843,  ...,  0.2000,  0.1529,  0.1373],\n",
      "          [ 0.5922,  0.5765,  0.5608,  ...,  0.2235,  0.1765,  0.1373]],\n",
      "\n",
      "         [[ 0.0902, -0.0588, -0.1765,  ..., -0.5137, -0.5686, -0.4667],\n",
      "          [ 0.0902, -0.0353, -0.1373,  ..., -0.5059, -0.5137, -0.4431],\n",
      "          [ 0.0745, -0.0118, -0.0902,  ..., -0.3647, -0.4039, -0.3569],\n",
      "          ...,\n",
      "          [ 0.6235,  0.6078,  0.6000,  ...,  0.2000,  0.1608,  0.1451],\n",
      "          [ 0.6078,  0.5922,  0.5843,  ...,  0.2000,  0.1529,  0.1373],\n",
      "          [ 0.5922,  0.5765,  0.5608,  ...,  0.2235,  0.1765,  0.1373]],\n",
      "\n",
      "         [[ 0.0902, -0.0588, -0.1765,  ..., -0.5137, -0.5686, -0.4667],\n",
      "          [ 0.0902, -0.0353, -0.1373,  ..., -0.5059, -0.5137, -0.4431],\n",
      "          [ 0.0745, -0.0118, -0.0902,  ..., -0.3647, -0.4039, -0.3569],\n",
      "          ...,\n",
      "          [ 0.6235,  0.6078,  0.6000,  ...,  0.2000,  0.1608,  0.1451],\n",
      "          [ 0.6078,  0.5922,  0.5843,  ...,  0.2000,  0.1529,  0.1373],\n",
      "          [ 0.5922,  0.5765,  0.5608,  ...,  0.2235,  0.1765,  0.1373]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0902, -0.0588, -0.1765,  ..., -0.2627, -0.2863, -0.2941],\n",
      "          [ 0.0902, -0.0353, -0.1373,  ..., -0.2235, -0.2549, -0.2706],\n",
      "          [ 0.0745, -0.0118, -0.0902,  ..., -0.1137, -0.1686, -0.1686],\n",
      "          ...,\n",
      "          [ 0.6235,  0.6078,  0.6000,  ...,  0.2000,  0.1529,  0.1451],\n",
      "          [ 0.6078,  0.5922,  0.5843,  ...,  0.1922,  0.1529,  0.1373],\n",
      "          [ 0.5843,  0.5686,  0.5529,  ...,  0.2157,  0.1765,  0.1373]],\n",
      "\n",
      "         [[ 0.0902, -0.0588, -0.1765,  ..., -0.2627, -0.2863, -0.2941],\n",
      "          [ 0.0902, -0.0353, -0.1373,  ..., -0.2235, -0.2549, -0.2706],\n",
      "          [ 0.0745, -0.0118, -0.0902,  ..., -0.1137, -0.1686, -0.1686],\n",
      "          ...,\n",
      "          [ 0.6235,  0.6078,  0.6000,  ...,  0.2000,  0.1529,  0.1451],\n",
      "          [ 0.6078,  0.5922,  0.5843,  ...,  0.1922,  0.1529,  0.1373],\n",
      "          [ 0.5843,  0.5686,  0.5529,  ...,  0.2157,  0.1765,  0.1373]],\n",
      "\n",
      "         [[ 0.0902, -0.0588, -0.1765,  ..., -0.2627, -0.2863, -0.2941],\n",
      "          [ 0.0902, -0.0353, -0.1373,  ..., -0.2235, -0.2549, -0.2706],\n",
      "          [ 0.0745, -0.0118, -0.0902,  ..., -0.1137, -0.1686, -0.1686],\n",
      "          ...,\n",
      "          [ 0.6235,  0.6078,  0.6000,  ...,  0.2000,  0.1529,  0.1451],\n",
      "          [ 0.6078,  0.5922,  0.5843,  ...,  0.1922,  0.1529,  0.1373],\n",
      "          [ 0.5843,  0.5686,  0.5529,  ...,  0.2157,  0.1765,  0.1373]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2784,  0.1765,  0.0588,  ..., -0.3725, -0.4510, -0.3490],\n",
      "          [ 0.2941,  0.2157,  0.1137,  ..., -0.3647, -0.3804, -0.3020],\n",
      "          [ 0.3020,  0.2549,  0.1608,  ..., -0.2314, -0.2706, -0.2314],\n",
      "          ...,\n",
      "          [ 0.7725,  0.7647,  0.7569,  ...,  0.3255,  0.3804,  0.3961],\n",
      "          [ 0.7569,  0.7569,  0.7490,  ...,  0.3490,  0.3725,  0.3804],\n",
      "          [ 0.7412,  0.7412,  0.7333,  ...,  0.3647,  0.3725,  0.3725]],\n",
      "\n",
      "         [[ 0.2784,  0.1765,  0.0588,  ..., -0.3725, -0.4510, -0.3490],\n",
      "          [ 0.2941,  0.2157,  0.1137,  ..., -0.3647, -0.3804, -0.3020],\n",
      "          [ 0.3020,  0.2549,  0.1608,  ..., -0.2314, -0.2706, -0.2314],\n",
      "          ...,\n",
      "          [ 0.7725,  0.7647,  0.7569,  ...,  0.3255,  0.3804,  0.3961],\n",
      "          [ 0.7569,  0.7569,  0.7490,  ...,  0.3490,  0.3725,  0.3804],\n",
      "          [ 0.7412,  0.7412,  0.7333,  ...,  0.3647,  0.3725,  0.3725]],\n",
      "\n",
      "         [[ 0.2784,  0.1765,  0.0588,  ..., -0.3725, -0.4510, -0.3490],\n",
      "          [ 0.2941,  0.2157,  0.1137,  ..., -0.3647, -0.3804, -0.3020],\n",
      "          [ 0.3020,  0.2549,  0.1608,  ..., -0.2314, -0.2706, -0.2314],\n",
      "          ...,\n",
      "          [ 0.7725,  0.7647,  0.7569,  ...,  0.3255,  0.3804,  0.3961],\n",
      "          [ 0.7569,  0.7569,  0.7490,  ...,  0.3490,  0.3725,  0.3804],\n",
      "          [ 0.7412,  0.7412,  0.7333,  ...,  0.3647,  0.3725,  0.3725]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.2784,  0.1765,  0.0588,  ..., -0.2000, -0.2157, -0.2235],\n",
      "          [ 0.2941,  0.2157,  0.1137,  ..., -0.1608, -0.1843, -0.2000],\n",
      "          [ 0.3020,  0.2549,  0.1608,  ..., -0.0588, -0.1059, -0.1059],\n",
      "          ...,\n",
      "          [ 0.7725,  0.7647,  0.7569,  ...,  0.3255,  0.3804,  0.3961],\n",
      "          [ 0.7569,  0.7569,  0.7490,  ...,  0.3490,  0.3725,  0.3804],\n",
      "          [ 0.7412,  0.7412,  0.7333,  ...,  0.3647,  0.3725,  0.3647]],\n",
      "\n",
      "         [[ 0.2784,  0.1765,  0.0588,  ..., -0.2000, -0.2157, -0.2235],\n",
      "          [ 0.2941,  0.2157,  0.1137,  ..., -0.1608, -0.1843, -0.2000],\n",
      "          [ 0.3020,  0.2549,  0.1608,  ..., -0.0588, -0.1059, -0.1059],\n",
      "          ...,\n",
      "          [ 0.7725,  0.7647,  0.7569,  ...,  0.3255,  0.3804,  0.3961],\n",
      "          [ 0.7569,  0.7569,  0.7490,  ...,  0.3490,  0.3725,  0.3804],\n",
      "          [ 0.7412,  0.7412,  0.7333,  ...,  0.3647,  0.3725,  0.3647]],\n",
      "\n",
      "         [[ 0.2784,  0.1765,  0.0588,  ..., -0.2000, -0.2157, -0.2235],\n",
      "          [ 0.2941,  0.2157,  0.1137,  ..., -0.1608, -0.1843, -0.2000],\n",
      "          [ 0.3020,  0.2549,  0.1608,  ..., -0.0588, -0.1059, -0.1059],\n",
      "          ...,\n",
      "          [ 0.7725,  0.7647,  0.7569,  ...,  0.3255,  0.3804,  0.3961],\n",
      "          [ 0.7569,  0.7569,  0.7490,  ...,  0.3490,  0.3725,  0.3804],\n",
      "          [ 0.7412,  0.7412,  0.7333,  ...,  0.3647,  0.3725,  0.3647]]],\n",
      "\n",
      "\n",
      "        [[[-0.2157, -0.2784, -0.3569,  ..., -0.3882, -0.4588, -0.3569],\n",
      "          [-0.1843, -0.2392, -0.3098,  ..., -0.3961, -0.3961, -0.3176],\n",
      "          [-0.1686, -0.2078, -0.2627,  ..., -0.2627, -0.2941, -0.2549],\n",
      "          ...,\n",
      "          [ 0.3333,  0.3255,  0.3176,  ..., -0.0431, -0.0667, -0.0667],\n",
      "          [ 0.3176,  0.3176,  0.3098,  ..., -0.0980, -0.1059, -0.0902],\n",
      "          [ 0.3020,  0.3020,  0.2941,  ..., -0.1451, -0.1373, -0.1216]],\n",
      "\n",
      "         [[-0.2157, -0.2784, -0.3569,  ..., -0.3882, -0.4588, -0.3569],\n",
      "          [-0.1843, -0.2392, -0.3098,  ..., -0.3961, -0.3961, -0.3176],\n",
      "          [-0.1686, -0.2078, -0.2627,  ..., -0.2627, -0.2941, -0.2549],\n",
      "          ...,\n",
      "          [ 0.3333,  0.3255,  0.3176,  ..., -0.0431, -0.0667, -0.0667],\n",
      "          [ 0.3176,  0.3176,  0.3098,  ..., -0.0980, -0.1059, -0.0902],\n",
      "          [ 0.3020,  0.3020,  0.2941,  ..., -0.1451, -0.1373, -0.1216]],\n",
      "\n",
      "         [[-0.2157, -0.2784, -0.3569,  ..., -0.3882, -0.4588, -0.3569],\n",
      "          [-0.1843, -0.2392, -0.3098,  ..., -0.3961, -0.3961, -0.3176],\n",
      "          [-0.1686, -0.2078, -0.2627,  ..., -0.2627, -0.2941, -0.2549],\n",
      "          ...,\n",
      "          [ 0.3333,  0.3255,  0.3176,  ..., -0.0431, -0.0667, -0.0667],\n",
      "          [ 0.3176,  0.3176,  0.3098,  ..., -0.0980, -0.1059, -0.0902],\n",
      "          [ 0.3020,  0.3020,  0.2941,  ..., -0.1451, -0.1373, -0.1216]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.2157, -0.2784, -0.3569,  ..., -0.5608, -0.5686, -0.5765],\n",
      "          [-0.1843, -0.2392, -0.3098,  ..., -0.5137, -0.5373, -0.5529],\n",
      "          [-0.1686, -0.2078, -0.2627,  ..., -0.3804, -0.4431, -0.4275],\n",
      "          ...,\n",
      "          [ 0.3333,  0.3255,  0.3176,  ..., -0.0510, -0.0667, -0.0667],\n",
      "          [ 0.3176,  0.3176,  0.3098,  ..., -0.1059, -0.1059, -0.0902],\n",
      "          [ 0.2941,  0.3020,  0.2941,  ..., -0.1451, -0.1373, -0.1216]],\n",
      "\n",
      "         [[-0.2157, -0.2784, -0.3569,  ..., -0.5608, -0.5686, -0.5765],\n",
      "          [-0.1843, -0.2392, -0.3098,  ..., -0.5137, -0.5373, -0.5529],\n",
      "          [-0.1686, -0.2078, -0.2627,  ..., -0.3804, -0.4431, -0.4275],\n",
      "          ...,\n",
      "          [ 0.3333,  0.3255,  0.3176,  ..., -0.0510, -0.0667, -0.0667],\n",
      "          [ 0.3176,  0.3176,  0.3098,  ..., -0.1059, -0.1059, -0.0902],\n",
      "          [ 0.2941,  0.3020,  0.2941,  ..., -0.1451, -0.1373, -0.1216]],\n",
      "\n",
      "         [[-0.2157, -0.2784, -0.3569,  ..., -0.5608, -0.5686, -0.5765],\n",
      "          [-0.1843, -0.2392, -0.3098,  ..., -0.5137, -0.5373, -0.5529],\n",
      "          [-0.1686, -0.2078, -0.2627,  ..., -0.3804, -0.4431, -0.4275],\n",
      "          ...,\n",
      "          [ 0.3333,  0.3255,  0.3176,  ..., -0.0510, -0.0667, -0.0667],\n",
      "          [ 0.3176,  0.3176,  0.3098,  ..., -0.1059, -0.1059, -0.0902],\n",
      "          [ 0.2941,  0.3020,  0.2941,  ..., -0.1451, -0.1373, -0.1216]]]])]\n",
      "['Japan_v2_2_062266_s2_frame1.png']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample Batch: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 1. Expected size 4 but got size 3 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m seed_everything(args\u001b[38;5;241m.\u001b[39mseed)\n\u001b[1;32m      2\u001b[0m rank, gpu_num \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mrun_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgpu_num\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrank\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 320\u001b[0m, in \u001b[0;36mrun_inference\u001b[0;34m(args, gpu_num, gpu_no)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    318\u001b[0m     videos \u001b[38;5;241m=\u001b[39m videos\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m--> 320\u001b[0m batch_samples \u001b[38;5;241m=\u001b[39m \u001b[43mimage_guided_synthesis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvideos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnoise_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mddim_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mddim_eta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m                    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munconditional_guidance_scale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcfg_img\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mframe_stride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultiple_cond_cfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimestep_spacing\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mguidance_rescale\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;66;03m## save each example individually\u001b[39;00m\n\u001b[1;32m    324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m nn, samples \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(batch_samples):\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;66;03m## samples : [n_samples,c,t,h,w]\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[7], line 226\u001b[0m, in \u001b[0;36mimage_guided_synthesis\u001b[0;34m(model, prompts, videos, noise_shape, n_samples, ddim_steps, ddim_eta, unconditional_guidance_scale, cfg_img, fs, text_input, multiple_cond_cfg, loop, interp, timestep_spacing, guidance_rescale, **kwargs)\u001b[0m\n\u001b[1;32m    223\u001b[0m     cond_z0 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ddim_sampler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 226\u001b[0m     samples, _ \u001b[38;5;241m=\u001b[39m \u001b[43mddim_sampler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mS\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mddim_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mconditioning\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcond\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnoise_shape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43munconditional_guidance_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munconditional_guidance_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43munconditional_conditioning\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43meta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mddim_eta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mcfg_img\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg_img\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcond_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mx0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcond_z0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mfs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mtimestep_spacing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimestep_spacing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mguidance_rescale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mguidance_rescale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m                                    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;66;03m## reconstruct from latent to pixel space\u001b[39;00m\n\u001b[1;32m    244\u001b[0m batch_images \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mdecode_first_stage(samples, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39madditional_decode_kwargs)\n",
      "File \u001b[0;32m~/.conda/envs/tooncrafter/lib/python3.8/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/net/scratch/b08593hm/character-sheets/ToonCrafter/lvdm/models/samplers/ddim.py:113\u001b[0m, in \u001b[0;36mDDIMSampler.sample\u001b[0;34m(self, S, batch_size, shape, conditioning, callback, normals_sequence, img_callback, quantize_x0, eta, mask, x0, temperature, noise_dropout, score_corrector, corrector_kwargs, verbose, schedule_verbose, x_T, log_every_t, unconditional_guidance_scale, unconditional_conditioning, precision, fs, timestep_spacing, guidance_rescale, **kwargs)\u001b[0m\n\u001b[1;32m    110\u001b[0m     C, T, H, W \u001b[38;5;241m=\u001b[39m shape\n\u001b[1;32m    111\u001b[0m     size \u001b[38;5;241m=\u001b[39m (batch_size, C, T, H, W)\n\u001b[0;32m--> 113\u001b[0m samples, intermediates \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mddim_sampling\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconditioning\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mimg_callback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimg_callback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mquantize_denoised\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquantize_x0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mddim_use_original_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mnoise_dropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnoise_dropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mscore_corrector\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscore_corrector\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mcorrector_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcorrector_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mx_T\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx_T\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mlog_every_t\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_every_t\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43munconditional_guidance_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munconditional_guidance_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43munconditional_conditioning\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munconditional_conditioning\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mprecision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprecision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mfs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mguidance_rescale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mguidance_rescale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m                                            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m samples, intermediates\n",
      "File \u001b[0;32m~/.conda/envs/tooncrafter/lib/python3.8/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/net/scratch/b08593hm/character-sheets/ToonCrafter/lvdm/models/samplers/ddim.py:185\u001b[0m, in \u001b[0;36mDDIMSampler.ddim_sampling\u001b[0;34m(self, cond, shape, x_T, ddim_use_original_steps, callback, timesteps, quantize_denoised, mask, x0, img_callback, log_every_t, temperature, noise_dropout, score_corrector, corrector_kwargs, unconditional_guidance_scale, unconditional_conditioning, verbose, precision, fs, guidance_rescale, **kwargs)\u001b[0m\n\u001b[1;32m    179\u001b[0m         img_orig \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mq_sample(x0, ts)  \u001b[38;5;66;03m# TODO: deterministic forward pass? <ddim inversion>\u001b[39;00m\n\u001b[1;32m    180\u001b[0m     img \u001b[38;5;241m=\u001b[39m img_orig \u001b[38;5;241m*\u001b[39m mask \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1.\u001b[39m \u001b[38;5;241m-\u001b[39m mask) \u001b[38;5;241m*\u001b[39m img \u001b[38;5;66;03m# keep original & modify use img\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m outs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp_sample_ddim\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcond\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_original_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mddim_use_original_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mquantize_denoised\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquantize_denoised\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mnoise_dropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnoise_dropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscore_corrector\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscore_corrector\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mcorrector_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcorrector_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m                          \u001b[49m\u001b[43munconditional_guidance_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munconditional_guidance_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m                          \u001b[49m\u001b[43munconditional_conditioning\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munconditional_conditioning\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43mx0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43mfs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfs\u001b[49m\u001b[43m,\u001b[49m\u001b[43mguidance_rescale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mguidance_rescale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m                          \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    195\u001b[0m img, pred_x0 \u001b[38;5;241m=\u001b[39m outs\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m callback: callback(i)\n",
      "File \u001b[0;32m~/.conda/envs/tooncrafter/lib/python3.8/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/net/scratch/b08593hm/character-sheets/ToonCrafter/lvdm/models/samplers/ddim.py:221\u001b[0m, in \u001b[0;36mDDIMSampler.p_sample_ddim\u001b[0;34m(self, x, c, t, index, repeat_noise, use_original_steps, quantize_denoised, temperature, noise_dropout, score_corrector, corrector_kwargs, unconditional_guidance_scale, unconditional_conditioning, uc_type, conditional_guidance_scale_temporal, mask, x0, guidance_rescale, **kwargs)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;66;03m### do_classifier_free_guidance\u001b[39;00m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(c, torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(c, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m--> 221\u001b[0m         e_t_cond \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    222\u001b[0m         e_t_uncond \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mapply_model(x, t, unconditional_conditioning, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/net/scratch/b08593hm/character-sheets/ToonCrafter/lvdm/models/ddpm3d.py:745\u001b[0m, in \u001b[0;36mLatentDiffusion.apply_model\u001b[0;34m(self, x_noisy, t, cond, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     key \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mc_concat\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mconditioning_key \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconcat\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mc_crossattn\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    743\u001b[0m     cond \u001b[38;5;241m=\u001b[39m {key: cond}\n\u001b[0;32m--> 745\u001b[0m x_recon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_noisy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcond\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x_recon, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x_recon[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/.conda/envs/tooncrafter/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/tooncrafter/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/net/scratch/b08593hm/character-sheets/ToonCrafter/lvdm/models/ddpm3d.py:1264\u001b[0m, in \u001b[0;36mDiffusionWrapper.forward\u001b[0;34m(self, x, t, c_concat, c_crossattn, c_adm, s, mask, **kwargs)\u001b[0m\n\u001b[1;32m   1262\u001b[0m     xc \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([x] \u001b[38;5;241m+\u001b[39m c_concat, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   1263\u001b[0m     cc \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(c_crossattn, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m-> 1264\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdiffusion_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1265\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconditioning_key \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresblockcond\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m   1266\u001b[0m     cc \u001b[38;5;241m=\u001b[39m c_crossattn[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/.conda/envs/tooncrafter/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/tooncrafter/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/net/scratch/b08593hm/character-sheets/ToonCrafter/lvdm/modules/networks/openaimodel3d.py:596\u001b[0m, in \u001b[0;36mUNetModel.forward\u001b[0;34m(self, x, timesteps, context, features_adapter, fs, **kwargs)\u001b[0m\n\u001b[1;32m    594\u001b[0m h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmiddle_block(h, emb, context\u001b[38;5;241m=\u001b[39mcontext, batch_size\u001b[38;5;241m=\u001b[39mb)\n\u001b[1;32m    595\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_blocks:\n\u001b[0;32m--> 596\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    597\u001b[0m     h \u001b[38;5;241m=\u001b[39m module(h, emb, context\u001b[38;5;241m=\u001b[39mcontext, batch_size\u001b[38;5;241m=\u001b[39mb)\n\u001b[1;32m    598\u001b[0m h \u001b[38;5;241m=\u001b[39m h\u001b[38;5;241m.\u001b[39mtype(x\u001b[38;5;241m.\u001b[39mdtype)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 1. Expected size 4 but got size 3 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "seed_everything(args.seed)\n",
    "rank, gpu_num = 0, 1\n",
    "run_inference(args, gpu_num, rank)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (tooncrafter)",
   "language": "python",
   "name": "tooncrafter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
